{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from sklearn.metrics import classification_report\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\mvy48\\\\Downloads\\\\ml_combined_anoop-cc-gokul_07Dec19.txt\"\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "lines = lines[:200000]\n",
    "\n",
    "def label_sentences(sentences):\n",
    "    labeled_data = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        words = sentence.split()\n",
    "        labels = []\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                labels.append((word, 'S'))\n",
    "            else:\n",
    "                labels.append((word[0], 'B'))\n",
    "                for char in word[1:-1]:\n",
    "                    labels.append((char, 'I'))\n",
    "                labels.append((word[-1], 'E'))\n",
    "        labeled_data.append(labels)\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_lines = label_sentences(lines)\n",
    "\n",
    "sentences = [[char for char, label in sentence] for sentence in labeled_lines]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "vocab = list(word2vec_model.wv.index_to_key)\n",
    "embeddings = [word2vec_model.wv[word] for word in vocab]\n",
    "embedding_df = pd.DataFrame(embeddings, index=vocab)\n",
    "embedding_df.to_csv('word_embeddings.csv')\n",
    "\n",
    "# Convert Characters to Indices and Prepare Labels\n",
    "char_to_index = {char: idx for idx, char in enumerate(word2vec_model.wv.index_to_key, start=1)}\n",
    "char_to_index['PAD'] = 0\n",
    "\n",
    "label_to_index = {'B': 0, 'I': 1, 'E': 2, 'S': 3}\n",
    "pad_label = -1\n",
    "\n",
    "def prepare_data(labeled_sentences, char_to_index, label_to_index):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in labeled_sentences:\n",
    "        sentence_indices = [char_to_index[char] for char, label in sentence]\n",
    "        label_indices = [label_to_index[label] for char, label in sentence]\n",
    "        X.append(sentence_indices)\n",
    "        y.append(label_indices)\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_data(labeled_lines, char_to_index, label_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "MAX_LEN = 100\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding='post', value=char_to_index['PAD'])\n",
    "y_padded = pad_sequences(y, maxlen=MAX_LEN, padding='post', value=pad_label)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_padded, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with hyperparameters\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=len(char_to_index),\n",
    "        output_dim=hp.Int('embedding_dim', min_value=50, max_value=200, step=50),\n",
    "        input_length=MAX_LEN,\n",
    "        mask_zero=True\n",
    "    ))\n",
    "    model.add(Dropout(rate=hp.Float('embedding_dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=True,\n",
    "        recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    model.add(SeqSelfAttention(\n",
    "        attention_activation='sigmoid'\n",
    "    ))\n",
    "    model.add(TimeDistributed(Dense(len(label_to_index), activation='softmax')))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='hyperband_dir',\n",
    "    project_name='word_segmentation'\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter search\n",
    "tuner.search(X_train, np.expand_dims(y_train, -1), epochs=50, validation_split=0.1, callbacks=[stop_early, checkpoint, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "\n",
    "# Train the model with the optimal hyperparameters\n",
    "history = model.fit(X_train, np.expand_dims(y_train, -1), batch_size=64, epochs=10, validation_split=0.1, callbacks=[stop_early, checkpoint, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = tf.keras.models.load_model('best_model.h5', custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, np.expand_dims(y_test, -1))\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_true = np.squeeze(y_test, axis=-1)\n",
    "\n",
    "# Replace pad labels with -1 for evaluation\n",
    "y_true[y_true == pad_label] = -1\n",
    "y_pred[y_true == -1] = -1\n",
    "\n",
    "# Flatten and filter out padding labels\n",
    "y_true_flat = y_true[y_true != -1].flatten()\n",
    "y_pred_flat = y_pred[y_true != -1].flatten()\n",
    "\n",
    "# Classification Report\n",
    "label_names = ['B', 'I', 'E', 'S']\n",
    "print(classification_report(y_true_flat, y_pred_flat, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predictions\n",
    "def decode_predictions(preds, index_to_label):\n",
    "    return [[index_to_label[idx] for idx in sentence] for sentence in preds]\n",
    "\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "def reconstruct_sentence(chars, labels):\n",
    "    words = []\n",
    "    word = ''\n",
    "    for char, label in zip(chars, labels):\n",
    "        if label == 'B':\n",
    "            if word:\n",
    "                words.append(word)\n",
    "            word = char\n",
    "        elif label == 'I':\n",
    "            word += char\n",
    "        elif label == 'E':\n",
    "            word += char\n",
    "            words.append(word)\n",
    "            word = ''\n",
    "        elif label == 'S':\n",
    "            if word:\n",
    "                words.append(word)\n",
    "            words.append(char)\n",
    "            word = ''\n",
    "    if word:\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "def predict_sentence(sentence, model, char_to_index, index_to_label):\n",
    "    sentence_indices = [char_to_index.get(char, 0) for char in sentence]\n",
    "    sentence_padded = pad_sequences([sentence_indices], maxlen=MAX_LEN, padding='post', value=char_to_index['PAD'])\n",
    "    preds = model.predict(sentence_padded)\n",
    "    label_preds = decode_predictions(np.argmax(preds, axis=-1), index_to_label)\n",
    "    segmented_words = reconstruct_sentence(sentence, label_preds[0])\n",
    "    return segmented_words\n",
    "\n",
    "# Define the sentence to predict\n",
    "sentence_to_predict = \"ആഗ്രഹങ്ങൾസാക്ഷാത്കരിക്കാന്പഠിക്കണം\"\n",
    "\n",
    "# Make Prediction\n",
    "predicted_words = predict_sentence(sentence_to_predict, model, char_to_index, index_to_label)\n",
    "\n",
    "# Print the Segmented Sentence\n",
    "print(\"Segmented Sentence:\", ' '.join(predicted_words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
