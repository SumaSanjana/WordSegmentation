{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the Malayalam dataset\n",
    "dataset_path = r\"C:\\Users\\mvy48\\Downloads\\ml_combined_anoop-cc-gokul_07Dec19.txt\"\n",
    "\n",
    "# Read the dataset and limit to the first 200,000 sentences\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = f.read().splitlines()[:200000]\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.7\n",
    "split_index = int(train_ratio * len(dataset))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = dataset[:split_index]\n",
    "test_data = dataset[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression to detect Malayalam characters\n",
    "regex = re.compile(\"[^\\u0D00-\\u0D7F]\")\n",
    "\n",
    "def preprocess(data):\n",
    "    hidden_state_lst = []\n",
    "    data_lst = []\n",
    "\n",
    "    for sentence in data:\n",
    "        hidden_state = \"\"\n",
    "        words = []\n",
    "\n",
    "        for token in sentence.split():\n",
    "            token = regex.sub(\"\", token)\n",
    "            # Label the token according to its length\n",
    "            if len(token) == 1:\n",
    "                hidden_state += \"S\"\n",
    "            elif len(token) == 2:\n",
    "                hidden_state += \"BE\"\n",
    "            elif len(token) > 2:\n",
    "                hidden_state += \"B\" + (len(token) - 2) * \"I\" + \"E\"\n",
    "\n",
    "        # Remove the spaces between characters\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        for word in sentence:\n",
    "            # Verify if the word is a Malayalam character\n",
    "            word = regex.sub(\"\", word)\n",
    "            words.append(word)\n",
    "\n",
    "        if len(words) > 0:\n",
    "            data_lst.append(words)\n",
    "            states = [s for s in hidden_state]\n",
    "            hidden_state_lst.append(states)\n",
    "\n",
    "    return data_lst, hidden_state_lst\n",
    "\n",
    "train_data, train_hs = preprocess(train_data)\n",
    "test_data, test_hs = preprocess(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'B': Beginning character\n",
    "'I': Intermediate (internal) character\n",
    "'E': Ending character\n",
    "'S': Single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store the state count\n",
    "state_count = {}\n",
    "states = [\"B\", \"I\", \"E\", \"S\"]\n",
    "\n",
    "for state in states:\n",
    "    state_count[state] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the state count\n",
    "            state_count[train_hs[i][j]] += 1\n",
    "        state_count[train_hs[i][length - 1]] += 1\n",
    "\n",
    "total_states = sum(state_count.values())  # Get the total number of states in the sample\n",
    "start_prob = {}\n",
    "\n",
    "for state in states:\n",
    "    # Normalize the state count with the total number of states\n",
    "    start_prob[state] = state_count[state] / total_states\n",
    "\n",
    "# Initialize the transition probabilities\n",
    "trans_prob = {}\n",
    "\n",
    "for state in states:\n",
    "    trans_prob[state] = {}\n",
    "    for state_i in states:\n",
    "        trans_prob[state][state_i] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the transition probabilities\n",
    "            s_from = train_hs[i][j]\n",
    "            s_to = train_hs[i][j + 1]\n",
    "            trans_prob[s_from][s_to] += 1\n",
    "\n",
    "for i in states:\n",
    "    for j in states:\n",
    "        # Normalize the frequency of the transition with the state counts\n",
    "        trans_prob[i][j] /= float(state_count[i])\n",
    "\n",
    "# Initialize the emission probabilities\n",
    "emission_prob = {}\n",
    "\n",
    "# Get all the vocabulary in the corpus (train and test sets)\n",
    "vocab = list(set([word for sentence in train_data for word in sentence] +\n",
    "                 [word for sentence in test_data for word in sentence]))\n",
    "\n",
    "# Initialize the emission probabilities\n",
    "for state in states:\n",
    "    emission_prob[state] = {}\n",
    "    for word in vocab:\n",
    "        emission_prob[state][word] = 1\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    for j in range(length):\n",
    "        # Update the emission probabilities\n",
    "        obs = train_data[i][j]\n",
    "        hidden = train_hs[i][j]\n",
    "        emission_prob[hidden][obs] += 1\n",
    "\n",
    "for state in states:\n",
    "    for word in vocab:\n",
    "        if emission_prob[state][word] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Normalize the emission probabilities\n",
    "            emission_prob[state][word] /= float(state_count[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(obs):\n",
    "    obs = [i for i in obs if i]\n",
    "    if len(obs) > 0:\n",
    "        # Initialize a list of dictionary to store the probabilites\n",
    "        V = [{}]\n",
    "        for st in states:\n",
    "            # Append the initial probabilites\n",
    "            V[0][st] = {\"prob\": start_prob[st] * emission_prob[st].get(obs[0], 0), \"prev\": None}\n",
    "        for t in range(1, len(obs)):\n",
    "            # Append a dictionary to store the probabilities at time/step t\n",
    "            V.append({})\n",
    "            for st in states:\n",
    "                max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_prob[states[0]][st]\n",
    "                prev_st_selected = states[0]\n",
    "                for prev_st in states[1:]:\n",
    "                    # Calculate the probabilities of each state\n",
    "                    tr_prob = V[t - 1][prev_st][\"prob\"] * trans_prob[prev_st][st]\n",
    "                    if tr_prob > max_tr_prob:\n",
    "                        max_tr_prob = tr_prob\n",
    "                        prev_st_selected = prev_st\n",
    "\n",
    "                # Get the max probability at time/step t\n",
    "                max_prob = max_tr_prob * emission_prob[st].get(obs[t], 0)\n",
    "                V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "        path = []\n",
    "        max_prob = -float(\"inf\")\n",
    "        best_st = None\n",
    "        # Get most probable state and its backtrack\n",
    "        for st, data in V[-1].items():\n",
    "            if data[\"prob\"] > max_prob:\n",
    "                max_prob = data[\"prob\"]\n",
    "                best_st = st\n",
    "        path.append(best_st)\n",
    "        previous = best_st\n",
    "\n",
    "        # Follow the backtrack till the first observation\n",
    "        for t in range(len(V) - 2, -1, -1):\n",
    "            path.insert(0, V[t + 1][previous][\"prev\"])\n",
    "            previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "        # Return the path\n",
    "        return path\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "test_pred = []\n",
    "for obs in test_data:\n",
    "    # Label the sequence using Viterbi algorithm\n",
    "    test_pred.append(viterbi_decoding(obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segmentation(test_data, pred):\n",
    "    segmented = \"\"\n",
    "    i = 0  # Counter for the test data index\n",
    "    j = 0  # Counter for the prediction index\n",
    "    while i < len(test_data):\n",
    "        segmented += test_data[i]\n",
    "        # Check for Malayalam character\n",
    "        if test_data[i] > u\"\\u0D00\" and test_data[i] < u\"\\u0D7F\":\n",
    "            # Add space after the character if label\n",
    "            # is either \"E\" or \"S\"\n",
    "            if pred[j] in [\"E\", \"S\"]:\n",
    "                segmented += \" \"\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return segmented\n",
    "\n",
    "segmented = []\n",
    "for i in range(len(test_data)):\n",
    "    # Convert BIES to word segmentation\n",
    "    segmented.append(word_segmentation(test_data[i], test_pred[i]))\n",
    "\n",
    "# Save the segmented sentences to my_prediction.txt file\n",
    "with open(\"my_prediction.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for item in segmented:\n",
    "        fout.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "കീസ്റ്റോൺഎക്സെൽഡകോട്ടആക്സസ്എന്നീപദ്ധതികൾക്കാണ്പ്രസിഡന്റ്അനുമതിനൽകിയത്\n",
      "Gold Label:\n",
      "['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'E']\n",
      "Predicted Label:\n",
      "['I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'E', 'B', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'E', 'B', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I']\n",
      "Segmented Sentence:\n",
      "കീസ്റ്റോൺ എക് സെൽ ഡകോട്ട ആക് സസ് എന്നീപദ്ധതികൾ ക്കാണ് പ്രസിഡന്റ് അനുമതിനൽ കിയത്\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Look at random segmented sentence\n",
    "i = np.random.randint(0, len(test_data))  # Random index within the test data size\n",
    "print(\"Original Sentence:\")\n",
    "print(''.join(test_data[i]))  # Original sentence\n",
    "print(\"Gold Label:\")\n",
    "print(test_hs[i])  # True label\n",
    "print(\"Predicted Label:\")\n",
    "print(test_pred[i])  # Predicted label\n",
    "print(\"Segmented Sentence:\")\n",
    "print(segmented[i])  # Segmented sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: ഞാൻഇന്നുനിങ്ങളെഎങ്ങനെസഹായിക്കാനാകും\n",
      "Predicted Segmentation: ഞാൻ ഇന്നുനിങ്ങളെ എങ്ങനെ സഹായിക്കാനാകും \n"
     ]
    }
   ],
   "source": [
    "def predict_segmentation(input_sequence):\n",
    "    # Preprocess the input sequence\n",
    "    input_data = [list(input_sequence)]\n",
    "    \n",
    "    # Predict the segmentation\n",
    "    pred_segmentation = viterbi_decoding(input_data[0])\n",
    "    \n",
    "    # Convert BIES to word segmentation\n",
    "    segmented_sentence = word_segmentation(input_data[0], pred_segmentation)\n",
    "    \n",
    "    # Print the predicted segmentation\n",
    "    print(\"Input Sequence:\", input_sequence)\n",
    "    print(\"Predicted Segmentation:\", segmented_sentence)\n",
    "\n",
    "# Example usage\n",
    "input_sequence = \"ഞാൻഇന്നുനിങ്ങളെഎങ്ങനെസഹായിക്കാനാകും\"\n",
    "predict_segmentation(input_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for state B: 0.6279667579279399\n",
      "F1 score for state I: 0.9103146014683459\n",
      "F1 score for state E: 0.6284848255479902\n",
      "F1 score for state S: 0.6457575459640152\n",
      "Macro-F1 score: 0.7031309327270727\n",
      "Accuracy: 85.46%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "map_dict = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"E\": 2,\n",
    "    \"S\": 3\n",
    "}\n",
    "\n",
    "true = list(np.concatenate(test_hs).flat)\n",
    "pred = list(np.concatenate(test_pred).flat)\n",
    "\n",
    "k = len(np.unique(true))  # Number of classes\n",
    "result = np.zeros((k, k))  # Initialize the confusion matrix\n",
    "\n",
    "for i in range(len(true)):\n",
    "    # Calculate the confusion matrix\n",
    "    result[map_dict[true[i]]][map_dict[pred[i]]] += 1\n",
    "\n",
    "precision_B = result[0][0] / (result[0][0] + result[1][0] + result[2][0] + result[3][0])\n",
    "precision_I = result[1][1] / (result[0][1] + result[1][1] + result[2][1] + result[3][1])\n",
    "precision_E = result[2][2] / (result[0][2] + result[1][2] + result[2][2] + result[3][2])\n",
    "precision_S = result[3][3] / (result[0][3] + result[1][3] + result[2][3] + result[3][3])\n",
    "\n",
    "recall_B = result[0][0] / (result[0][0] + result[0][1] + result[0][2] + result[0][3])\n",
    "recall_I = result[1][1] / (result[1][0] + result[1][1] + result[1][2] + result[1][3])\n",
    "recall_E = result[2][2] / (result[2][0] + result[2][1] + result[2][2] + result[2][3])\n",
    "recall_S = result[3][3] / (result[3][0] + result[3][1] + result[3][2] + result[3][3])\n",
    "\n",
    "f1_B = 2 * (precision_B * recall_B) / (precision_B + recall_B)\n",
    "f1_I = 2 * (precision_I * recall_I) / (precision_I + recall_I)\n",
    "f1_E = 2 * (precision_E * recall_E) / (precision_E + recall_E)\n",
    "f1_S = 2 * (precision_S * recall_S) / (precision_S + recall_S)\n",
    "\n",
    "macro_f1 = (f1_B + f1_I + f1_E + f1_S) / k\n",
    "\n",
    "print(f\"F1 score for state B: {f1_B}\")\n",
    "print(f\"F1 score for state I: {f1_I}\")\n",
    "print(f\"F1 score for state E: {f1_E}\")\n",
    "print(f\"F1 score for state S: {f1_S}\")\n",
    "print(f\"Macro-F1 score: {macro_f1}\")\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for true, pred in zip(true, pred) if true == pred)\n",
    "total_predictions = len(true)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
